{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "yb__E0kn3Fvq"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import gc\n",
        "import glob\n",
        "import json\n",
        "import datetime\n",
        "from collections import defaultdict\n",
        "import multiprocessing as mp\n",
        "from pathlib import Path\n",
        "from types import SimpleNamespace\n",
        "from typing import Dict, List, Optional, Tuple\n",
        "import warnings\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "import numpy as np\n",
        "import PIL.Image as Image\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset\n",
        "from tqdm import tqdm\n",
        "import tifffile\n",
        "import random\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "import logging.handlers\n",
        "\n",
        "root_logger = logging.getLogger()\n",
        "root_logger.setLevel(logging.INFO)\n",
        "\n",
        "# Some libraries attempt to add their own root logger handlers. This is\n",
        "# annoying and so we get rid of them.\n",
        "for handler in list(root_logger.handlers):\n",
        "    root_logger.removeHandler(handler)\n",
        "\n",
        "logfmt_str = \"%(asctime)s %(levelname)-8s pid:%(process)d %(name)s:%(lineno)03d:%(funcName)s %(message)s\"\n",
        "formatter = logging.Formatter(logfmt_str)\n",
        "\n",
        "streamHandler = logging.StreamHandler()\n",
        "streamHandler.setFormatter(formatter)\n",
        "streamHandler.setLevel(logging.DEBUG)\n",
        "\n",
        "root_logger.addHandler(streamHandler)"
      ],
      "metadata": {
        "id": "qr7CMjZv6zlx"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive"
      ],
      "metadata": {
        "id": "N3TAND6m3a2h"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "00pEYPcX3qO5",
        "outputId": "f8da8f66-6743-4dcb-c6d6-f2b6028832df"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# def create_bounding_box(image):\n",
        "#     # Find the rows and columns where pixel values are equal to 1\n",
        "#     rows = np.any(image == 1, axis=1)\n",
        "#     cols = np.any(image == 1, axis=0)\n",
        "    \n",
        "#     # Find the indices of the first and last non-zero rows and columns\n",
        "#     rmin, rmax = np.where(rows)[0][[0, -1]]\n",
        "#     cmin, cmax = np.where(cols)[0][[0, -1]]\n",
        "    \n",
        "#     # Create the bounding box\n",
        "#     bounding_box = (rmin, rmax+1, cmin, cmax+1)\n",
        "    \n",
        "#     return bounding_box\n",
        "\n",
        "\n",
        "\n",
        "# def create_mask(image, bounding_box):\n",
        "#     # Create an empty mask with the same shape as the input image\n",
        "#     mask = np.zeros_like(image, dtype=np.bool)\n",
        "    \n",
        "#     # Unpack the bounding box coordinates\n",
        "#     rmin, rmax, cmin, cmax = bounding_box\n",
        "    \n",
        "#     # Set the values inside the bounding box to True\n",
        "#     mask[rmin:rmax, cmin:cmax] = True\n",
        "    \n",
        "#     return mask"
      ],
      "metadata": {
        "id": "MU-o6x1kGZiE"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Ind_ID:\n",
        "    def __init__(self, mode='test', id=None, size=None):\n",
        "\n",
        "        assert mode in ['train', 'test'], \"mode must be either 'train' or 'test'\"\n",
        "\n",
        "        self.id = str(id)\n",
        "        self.mode = mode\n",
        "        self.size = size if isinstance(size, tuple) else (size, size)\n",
        "        self.pos_label = None\n",
        "        \n",
        "        id_path = glob.glob('/content/drive/MyDrive/Data/Vesuvisius/{}/{}'.format(self.mode, self.id))[0]\n",
        "        print(id_path)\n",
        "\n",
        "        image_paths = sorted(glob.glob(os.path.join(id_path, 'surface_volume', '*.tif')))\n",
        "        images = []\n",
        "        for image_path in image_paths:\n",
        "            image = tifffile.imread(image_path)\n",
        "            \n",
        "            if self.size:\n",
        "                image = self.resize_image(image, self.size).astype(np.float32)\n",
        "            else:\n",
        "                image = image.astype(np.float32)\n",
        "                \n",
        "            images.append(image)\n",
        "        self.stacked = np.stack(images, axis=0)\n",
        "        \n",
        "        self.pos_mask = self.create_mask(self.stacked)\n",
        "        \n",
        "        # self.pos_mask = np.expand_dims(self.pos_mask, axis=0)\n",
        "        # self.pos_mask = np.repeat(self.pos_mask, self.stacked.shape[0], axis=0)\n",
        "        # print(self.pos_mask)\n",
        "        self.positive_indexes = (self.pos_mask.sum(axis=(1,2))\n",
        "                                 .nonzero()[0].tolist())\n",
        "        print(self.positive_indexes)\n",
        "        if self.mode == 'train':\n",
        "            label_path = os.path.join(id_path, 'inklabels.png')\n",
        "            assert os.path.exists(label_path), \"inklabels.png does not exist in the folder\"\n",
        "\n",
        "            self.pos_label = self.load_label(label_path)\n",
        "            print(self.pos_label)\n",
        "            self.pos_label = np.expand_dims(self.pos_label, axis=0)\n",
        "            self.pos_label = np.repeat(self.pos_label, self.stacked.shape[0], axis=0)\n",
        "            # print(self.pos_label.shape)\n",
        "            self.positive_indexes_lbl = (self.pos_label.sum(axis=(1,2)).nonzero()[0].tolist())\n",
        "        # else:\n",
        "        #     self.pos_label = None\n",
        "        #     self.positive_indexes_lbl = None\n",
        "        #     print(\"Warning: 'inklabel.png' file will not be used in the 'test' mode\")\n",
        "        \n",
        "    def resize_image(self, image, size):\n",
        "        # Convert the image to a supported data type\n",
        "        image = image.astype(np.uint8)\n",
        "        # Convert the image to a PIL Image\n",
        "        pil_image = Image.fromarray(image)\n",
        "        # Resize the image using the specified size\n",
        "        pil_image = pil_image.resize(size, Image.ANTIALIAS)\n",
        "        # Convert the PIL Image back to a numpy array\n",
        "        image = np.array(pil_image, dtype=bool)\n",
        "        return image\n",
        "\n",
        "    def load_label(self, label_path):\n",
        "        label = Image.open(label_path)\n",
        "\n",
        "        binary_label = label.convert('1')\n",
        "\n",
        "        # Convert the binary mask to a numpy array\n",
        "        binary_label = np.array(binary_label).astype(int)\n",
        "\n",
        "        # Convert the values to [False, True]\n",
        "        binary_label = binary_label.astype(bool)\n",
        "\n",
        "        if self.size:\n",
        "            binary_label = self.resize_image(binary_label, self.size)\n",
        "        else:\n",
        "            binary_label = np.array(binary_label).astype(np.float32)\n",
        "\n",
        "        return binary_label\n",
        "\n",
        "    def load_mask(self, path):\n",
        "        # id_path = glob.glob('/content/drive/MyDrive/Data/Vesuvisius/{}/{}'.format(self.mode, self.id))[0]\n",
        "\n",
        "        mask = Image.open(self.path)\n",
        "\n",
        "        binary_mask = mask.convert('1')\n",
        "\n",
        "        # Convert the binary mask to a numpy array\n",
        "        binary_mask = np.array(binary_mask).astype(bool)\n",
        "        \n",
        "        if self.size:\n",
        "            binary_mask = self.resize_image(binary_mask, self.size)\n",
        "        else:\n",
        "            binary_mask = np.array(binary_mask).astype(np.float32)\n",
        "        \n",
        "        return binary_mask\n",
        "\n",
        "\n",
        "    def create_mask(self, image):\n",
        "        # Find the rows and columns where pixel values are equal to 1\n",
        "        rows = np.any(image == 1, axis=1)\n",
        "        cols = np.any(image == 1, axis=0)\n",
        "\n",
        "        # Find the indices of the first and last non-zero rows and columns\n",
        "        rmin, rmax = np.where(rows)[0][[0, -1]]\n",
        "        cmin, cmax = np.where(cols)[0][[0, -1]]\n",
        "\n",
        "        # Create an empty mask with the same shape as the input image\n",
        "        mask = np.zeros_like(image, dtype='bool')\n",
        "\n",
        "        # Set the values inside the bounding box to True\n",
        "        mask[rmin:rmax+1, cmin:cmax+1] = True\n",
        "        \n",
        "        return mask\n",
        "\n",
        "    def get_raw_candidate(self, index):\n",
        "        # Get the image and mask at the specified index\n",
        "        image = self.stacked[index]\n",
        "        mask = self.pos_mask[index]\n",
        "\n",
        "        # if hasattr(self, 'pos_label'):\n",
        "        label = self.pos_label[index]\n",
        "        return image, mask, label\n",
        "        # else:\n",
        "        #     return image, mask\n",
        "\n",
        "\n",
        "print(Ind_ID('train', '1', (512, 512)))\n"
      ],
      "metadata": {
        "id": "ZQ5ELuZW3u5_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13974318-024d-4770-eb0f-2563ff0ec233"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Data/Vesuvisius/train/1\n",
            "[0, 1, 2, 3, 4]\n",
            "[[False False False ... False False False]\n",
            " [False False False ... False False False]\n",
            " [False False False ... False False False]\n",
            " ...\n",
            " [False False False ... False False False]\n",
            " [False False False ... False False False]\n",
            " [False False False ... False False False]]\n",
            "<__main__.Ind_ID object at 0x7ff0d1ac2cb0>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_Id_list(mode):\n",
        "    path = '/content/drive/MyDrive/Data/Vesuvisius/{}'.format(mode)\n",
        "    files = os.listdir(path)\n",
        "    return files\n",
        "\n",
        "def getCt(mode, id, size):\n",
        "    return Ind_ID(mode, id, size)\n",
        "\n",
        "\n",
        "def getSampleSize(mode, id, size):\n",
        "    ct = Ind_ID(mode, id, size)\n",
        "    return int(ct.stacked.shape[0]), ct.positive_indexes\n",
        "\n",
        "print(getSampleSize('train', '1', (512, 512)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kjONZoUMMNyt",
        "outputId": "ac3bd234-ff1d-4d0d-837c-cd6342634295"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Data/Vesuvisius/train/1\n",
            "[0, 1, 2, 3, 4]\n",
            "[[False False False ... False False False]\n",
            " [False False False ... False False False]\n",
            " [False False False ... False False False]\n",
            " ...\n",
            " [False False False ... False False False]\n",
            " [False False False ... False False False]\n",
            " [False False False ... False False False]]\n",
            "(5, [0, 1, 2, 3, 4])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2"
      ],
      "metadata": {
        "id": "GN_mf5PyDZ3_"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ValidationSegmentationDataset(Dataset):\n",
        "    def __init__(self, mode='test', id=None, size=None, context_slices = 5, full_bool = False):\n",
        "        assert mode in ['train', 'test'], \"mode must be either 'train' or 'test'\"\n",
        "        \n",
        "        self.mode = mode\n",
        "        self.context_slices = context_slices\n",
        "        self.full_bool = full_bool\n",
        "        self.size = size\n",
        "\n",
        "        # if isinstance(id, int):\n",
        "        #     self.id = str(id)\n",
        "        # else:\n",
        "        #     self.id = id\n",
        "\n",
        "        if id:\n",
        "          self.id_list = [id]\n",
        "        else:\n",
        "          self.id_list = sorted(get_Id_list(self.mode))\n",
        "\n",
        "        if self.mode == 'train':\n",
        "          self.id_list = sorted(get_Id_list(self.mode))\n",
        "        elif self.mode == 'test':\n",
        "          self.id_list = sorted(get_Id_list(self.mode))\n",
        "\n",
        "\n",
        "        \n",
        "        self.sample_list = []\n",
        "\n",
        "        for id in self.id_list:\n",
        "          index_ct, pos_indx = getSampleSize(self.mode, id, self.size)\n",
        "\n",
        "          if self.full_bool:\n",
        "                self.sample_list += [(id, slice_ndx)\n",
        "                                     for slice_ndx in range(index_ct)]\n",
        "          else:\n",
        "                self.sample_list += [(id, slice_ndx)\n",
        "                                     for slice_ndx in pos_indx]\n",
        "        # print(self.id_list)\n",
        "        # print(self.sample_list)\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.sample_list)\n",
        "\n",
        "    def __getitem__(self, ndx):\n",
        "        series_uid, slice_ndx = self.sample_list[ndx % len(self.sample_list)]\n",
        "        print(\"wow\", series_uid, slice_ndx)\n",
        "        return self.getitem_fullSlice(series_uid, slice_ndx)\n",
        "\n",
        "    def getitem_fullSlice(self, id, slice_ndx):\n",
        "        ct = getCt(self.mode, id, self.size)\n",
        "        ct_t = torch.zeros((self.context_slices * 1, 512, 512))\n",
        "\n",
        "        start_ndx = (slice_ndx - self.context_slices) *(-1)\n",
        "        end_ndx = slice_ndx + self.context_slices + 1\n",
        "        print((start_ndx, end_ndx))\n",
        "\n",
        "        for i, context_ndx in enumerate(range(start_ndx, end_ndx)):\n",
        "            context_ndx = max(context_ndx, 0)\n",
        "            context_ndx = min(context_ndx, ct.stacked.shape[0] - 1)\n",
        "            # ct_t[i] = torch.from_numpy(ct.stacked[context_ndx].astype(np.float32))\n",
        "\n",
        "            # Check the shape of the stacked attribute\n",
        "            stacked_shape = ct.stacked[context_ndx].shape\n",
        "            if stacked_shape != (512, 512):\n",
        "                # Resize the stacked attribute if necessary\n",
        "                stacked_resized = cv2.resize(ct.stacked[context_ndx], (512, 512))\n",
        "                # print(\"&&******&&&&&&&\", stacked_resized.shape)\n",
        "\n",
        "                ct_t[i] = torch.from_numpy(stacked_resized.astype(np.float32))\n",
        "            else:\n",
        "                print(\"&&******&&&&&&&\", ct.stacked[context_ndx].shape)\n",
        "                ct_t[i] = torch.from_numpy(ct.stacked[context_ndx].astype(np.float32))\n",
        "\n",
        "        pos_t = torch.from_numpy(ct.pos_mask[slice_ndx]).unsqueeze(0)\n",
        "        if ct.pos_label is not None:\n",
        "            ink_t = torch.from_numpy(ct.pos_label[slice_ndx]).unsqueeze(0)\n",
        "        else:\n",
        "            ink_t = None\n",
        "\n",
        "        if ink_t is not None:\n",
        "            return ct_t, pos_t, ink_t, ct.id, slice_ndx\n",
        "        else:\n",
        "            return ct_t, pos_t, ct.id, slice_ndx\n",
        "        \n",
        "    \n",
        "\n",
        "class TrainingSegmentationDataset(ValidationSegmentationDataset):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        # self.size = (256, 256)\n",
        "\n",
        "\n",
        "    def __getitem__(self, ndx):\n",
        "        series_uid, slice_ndx = self.sample_list[ndx % len(self.sample_list)]\n",
        "        \n",
        "        return self.getitem_trainCrop(series_uid, slice_ndx)  \n",
        "    \n",
        "    def getitem_trainCrop(self, id, slice_ndx):\n",
        "        ct = getCt(self.mode, id, self.size)\n",
        "        ct_t = torch.zeros((self.context_slices * 2 - 1, 256, 256))\n",
        "        \n",
        "        start_ndx = slice_ndx - self.context_slices\n",
        "        end_ndx = start_ndx + 5\n",
        "        \n",
        "        for i, context_ndx in enumerate(range(start_ndx+1, end_ndx+1)):\n",
        "            context_ndx = max(context_ndx, 0)\n",
        "            context_ndx = min(context_ndx, ct.stacked.shape[0] - 1)\n",
        "\n",
        "            stacked_shape = ct.stacked[context_ndx].shape\n",
        "            if stacked_shape != (256, 256):\n",
        "                stacked_resized = cv2.resize(ct.stacked[context_ndx], (256, 256))\n",
        "                print(\"stacked_resized \", stacked_resized.shape)\n",
        "                ct_t[i] = torch.from_numpy(stacked_resized.astype(np.float32))\n",
        "            else:\n",
        "                ct_t[i] = torch.from_numpy(ct.stacked[context_ndx].astype(np.float32))\n",
        "\n",
        "        pos_t = torch.from_numpy(ct.pos_mask[slice_ndx].astype(np.float32)).unsqueeze(0)\n",
        "        \n",
        "        if ct.pos_label is not None:\n",
        "            ink_t = torch.from_numpy(ct.pos_label[slice_ndx].astype(np.float32)).unsqueeze(0)\n",
        "        else:\n",
        "            ink_t = None\n",
        "\n",
        "        if ink_t is not None:\n",
        "            return ct_t, pos_t, ink_t, ct.id, slice_ndx\n",
        "        else:\n",
        "            return ct_t, pos_t, ct.id, slice_ndx\n",
        "\n",
        "    \n",
        "        \n",
        "\n"
      ],
      "metadata": {
        "id": "hMvKRWuetEJm"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir = \"/content/drive/MyDrive/Data/Vesuvisius\"\n",
        "\n",
        "augmentation_dict = {'flip': True, 'offset': 0.1, 'scale': 0.2, 'rotate': True, 'noise': 25.0}\n",
        "# Create a SegmentationDataset object\n",
        "# test_dataset= ValidationSegmentationDataset('test', 'a', (512, 512), 5, True)\n",
        "# print(test_dataset[0][0].shape)\n",
        "train_dataset = TrainingSegmentationDataset('train', '1', (256, 256), 5, True)\n",
        "print(train_dataset[0][0].shape)\n",
        "\n",
        "# for i in train_dataset:\n",
        "#   print(i)\n",
        "\n",
        "\n",
        "# Use the dataset object to access a fragment\n",
        "# fragment = dataset[image][0][1]\n",
        "# print(fragment.shape)\n",
        "# Use indexing to access an individual image within the fragment\n",
        "# image = fragment\n",
        "# print(torch.max(image).item())\n",
        "# mask = image > 0\n",
        "\n",
        "# # Use the mask to index the image tensor and get the elements that are greater than 0\n",
        "# image_gt_0 = image[mask]\n",
        "\n",
        "# # Print the resulting tensor\n",
        "# print(image_gt_0)\n",
        "\n",
        "# Display the image using a library such as matplotlib\n",
        "# import matplotlib.pyplot as plt\n",
        "# plt.imshow(image, cmap='gray')\n",
        "# plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ORRogn7n8q_J",
        "outputId": "fd92f580-02f3-4f13-db58-d422cdfe447a"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Data/Vesuvisius/train/1\n",
            "[0, 1, 2, 3, 4]\n",
            "[[False False False ... False False False]\n",
            " [False False False ... False False False]\n",
            " [False False False ... False False False]\n",
            " ...\n",
            " [False False False ... False False False]\n",
            " [False False False ... False False False]\n",
            " [False False False ... False False False]]\n",
            "/content/drive/MyDrive/Data/Vesuvisius/train/2\n",
            "[0, 1, 2, 3, 4]\n",
            "[[False False False ... False False False]\n",
            " [False False False ... False False False]\n",
            " [False False False ... False False False]\n",
            " ...\n",
            " [False False False ... False False False]\n",
            " [False False False ... False False False]\n",
            " [False False False ... False False False]]\n",
            "/content/drive/MyDrive/Data/Vesuvisius/train/3\n",
            "[0, 1, 2, 3, 4]\n",
            "[[False False False ... False False False]\n",
            " [False False False ... False False False]\n",
            " [False False False ... False False False]\n",
            " ...\n",
            " [False False False ... False False False]\n",
            " [False False False ... False False False]\n",
            " [False False False ... False False False]]\n",
            "/content/drive/MyDrive/Data/Vesuvisius/train/1\n",
            "[0, 1, 2, 3, 4]\n",
            "[[False False False ... False False False]\n",
            " [False False False ... False False False]\n",
            " [False False False ... False False False]\n",
            " ...\n",
            " [False False False ... False False False]\n",
            " [False False False ... False False False]\n",
            " [False False False ... False False False]]\n",
            "torch.Size([9, 256, 256])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# for i, sample in enumerate(dataset):\n",
        "#             print(f\"Sample {i}: {sample}\")"
      ],
      "metadata": {
        "id": "eWKyj_eS2c2W"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# labels, inklabels = dataset[0][2]\n",
        "# print(torch.min(inklabels).item())\n",
        "# plt.imshow(inklabels, cmap='gray')\n",
        "# plt.show()\n"
      ],
      "metadata": {
        "id": "fVSF0BkSoIRY"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8v3hRdfk5nbQ"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, in_channels=1, n_classes=2, depth=5, wf=6, padding=False,\n",
        "                 batch_norm=False, up_mode='upconv'):\n",
        "        \"\"\"\n",
        "        Implementation of\n",
        "        U-Net: Convolutional Networks for Biomedical Image Segmentation\n",
        "        (Ronneberger et al., 2015)\n",
        "        https://arxiv.org/abs/1505.04597\n",
        "        Using the default arguments will yield the exact version used\n",
        "        in the original paper\n",
        "        Args:\n",
        "            in_channels (int): number of input channels\n",
        "            n_classes (int): number of output channels\n",
        "            depth (int): depth of the network\n",
        "            wf (int): number of filters in the first layer is 2**wf\n",
        "            padding (bool): if True, apply padding such that the input shape\n",
        "                            is the same as the output.\n",
        "                            This may introduce artifacts\n",
        "            batch_norm (bool): Use BatchNorm after layers with an\n",
        "                               activation function\n",
        "            up_mode (str): one of 'upconv' or 'upsample'.\n",
        "                           'upconv' will use transposed convolutions for\n",
        "                           learned upsampling.\n",
        "                           'upsample' will use bilinear upsampling.\n",
        "        \"\"\"\n",
        "        super(UNet, self).__init__()\n",
        "        assert up_mode in ('upconv', 'upsample')\n",
        "        self.padding = padding\n",
        "        self.depth = depth\n",
        "        prev_channels = in_channels\n",
        "        self.down_path = nn.ModuleList()\n",
        "        for i in range(depth):\n",
        "            self.down_path.append(UNetConvBlock(prev_channels, 2**(wf+i),\n",
        "                                                padding, batch_norm))\n",
        "            prev_channels = 2**(wf+i)\n",
        "\n",
        "        self.up_path = nn.ModuleList()\n",
        "        for i in reversed(range(depth - 1)):\n",
        "            self.up_path.append(UNetUpBlock(prev_channels, 2**(wf+i), up_mode,\n",
        "                                            padding, batch_norm))\n",
        "            prev_channels = 2**(wf+i)\n",
        "\n",
        "        self.last = nn.Conv2d(prev_channels, n_classes, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        blocks = []\n",
        "        for i, down in enumerate(self.down_path):\n",
        "            x = down(x)\n",
        "            if i != len(self.down_path)-1:\n",
        "                blocks.append(x)\n",
        "                x = F.avg_pool2d(x, 2)\n",
        "\n",
        "        for i, up in enumerate(self.up_path):\n",
        "            x = up(x, blocks[-i-1])\n",
        "\n",
        "        return self.last(x)\n",
        "\n",
        "\n",
        "class UNetConvBlock(nn.Module):\n",
        "    def __init__(self, in_size, out_size, padding, batch_norm):\n",
        "        super(UNetConvBlock, self).__init__()\n",
        "        block = []\n",
        "\n",
        "        block.append(nn.Conv2d(in_size, out_size, kernel_size=3,\n",
        "                               padding=int(padding)))\n",
        "        block.append(nn.ReLU())\n",
        "        # block.append(nn.LeakyReLU())\n",
        "        if batch_norm:\n",
        "            block.append(nn.BatchNorm2d(out_size))\n",
        "\n",
        "        block.append(nn.Conv2d(out_size, out_size, kernel_size=3,\n",
        "                               padding=int(padding)))\n",
        "        block.append(nn.ReLU())\n",
        "        # block.append(nn.LeakyReLU())\n",
        "        if batch_norm:\n",
        "            block.append(nn.BatchNorm2d(out_size))\n",
        "\n",
        "        self.block = nn.Sequential(*block)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.block(x)\n",
        "        return out\n",
        "\n",
        "\n",
        "class UNetUpBlock(nn.Module):\n",
        "    def __init__(self, in_size, out_size, up_mode, padding, batch_norm):\n",
        "        super(UNetUpBlock, self).__init__()\n",
        "        if up_mode == 'upconv':\n",
        "            self.up = nn.ConvTranspose2d(in_size, out_size, kernel_size=2,\n",
        "                                         stride=2)\n",
        "        elif up_mode == 'upsample':\n",
        "            self.up = nn.Sequential(nn.Upsample(mode='bilinear', scale_factor=2),\n",
        "                                    nn.Conv2d(in_size, out_size, kernel_size=1))\n",
        "\n",
        "        self.conv_block = UNetConvBlock(in_size, out_size, padding, batch_norm)\n",
        "\n",
        "    def center_crop(self, layer, target_size):\n",
        "        _, _, layer_height, layer_width = layer.size()\n",
        "        diff_y = (layer_height - target_size[0]) // 2\n",
        "        diff_x = (layer_width - target_size[1]) // 2\n",
        "        return layer[:, :, diff_y:(diff_y + target_size[0]), diff_x:(diff_x + target_size[1])]\n",
        "\n",
        "    def forward(self, x, bridge):\n",
        "        up = self.up(x)\n",
        "        crop1 = self.center_crop(bridge, up.shape[2:])\n",
        "        out = torch.cat([up, crop1], 1)\n",
        "        out = self.conv_block(out)\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "eYKOdg3hOy5V"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import random\n",
        "from collections import namedtuple\n",
        "\n",
        "import torch\n",
        "from torch import nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as transforms\n",
        "# from util.logconf import logging\n",
        "# from util.unet import UNet\n",
        "\n",
        "log = logging.getLogger(__name__)\n",
        "log.setLevel(logging.WARN)\n",
        "log.setLevel(logging.INFO)\n",
        "log.setLevel(logging.DEBUG)\n",
        "\n",
        "class UNetWrapper(nn.Module):\n",
        "    def __init__(self, **kwargs):\n",
        "        super().__init__()\n",
        "\n",
        "        self.input_batchnorm = nn.BatchNorm2d(kwargs['in_channels'])\n",
        "        self.unet = UNet(**kwargs)\n",
        "        self.final = nn.Sigmoid()\n",
        "\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        init_set = {\n",
        "            nn.Conv2d,\n",
        "            nn.Conv3d,\n",
        "            nn.ConvTranspose2d,\n",
        "            nn.ConvTranspose3d,\n",
        "            nn.Linear,\n",
        "        }\n",
        "        for m in self.modules():\n",
        "            if type(m) in init_set:\n",
        "                nn.init.kaiming_normal_(\n",
        "                    m.weight.data, mode='fan_out', nonlinearity='relu', a=0\n",
        "                )\n",
        "                if m.bias is not None:\n",
        "                    fan_in, fan_out = \\\n",
        "                        nn.init._calculate_fan_in_and_fan_out(m.weight.data)\n",
        "                    bound = 1 / math.sqrt(fan_out)\n",
        "                    nn.init.normal_(m.bias, -bound, bound)\n",
        "\n",
        "        # nn.init.constant_(self.unet.last.bias, -4)\n",
        "        # nn.init.constant_(self.unet.last.bias, 4)\n",
        "\n",
        "\n",
        "    def forward(self, input_batch):\n",
        "        bn_output = self.input_batchnorm(input_batch)\n",
        "        un_output = self.unet(bn_output)\n",
        "        fn_output = self.final(un_output)\n",
        "        return fn_output\n",
        "\n",
        "class SegmentationAugmentation(nn.Module):\n",
        "    def __init__(\n",
        "            self, flip=None, offset=None, scale=None, rotate=None, noise=None\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.flip = flip\n",
        "        self.offset = offset\n",
        "        self.scale = scale\n",
        "        self.rotate = rotate\n",
        "        self.noise = noise\n",
        "\n",
        "    def forward(self, input_g, label_g, inklabel_g=None):\n",
        "        transform_t = self._build2dTransformMatrix()\n",
        "        transform_t = transform_t.expand(input_g.shape[0], -1, -1)\n",
        "        transform_t = transform_t.to(input_g.device, torch.float32)\n",
        "        affine_t = F.affine_grid(transform_t[:,:2],\n",
        "                input_g.size(), align_corners=False)\n",
        "\n",
        "        augmented_input_g = F.grid_sample(input_g,\n",
        "                affine_t, padding_mode='border',\n",
        "                align_corners=False)\n",
        "        augmented_label_g = F.grid_sample(label_g.to(torch.float32),\n",
        "                affine_t, padding_mode='border',\n",
        "                align_corners=False)\n",
        "        if inklabel_g is not None:\n",
        "            augmented_inklabel_g = F.grid_sample(inklabel_g.to(torch.float32),\n",
        "                affine_t, padding_mode='border', \n",
        "                align_corners=False)\n",
        "        else:\n",
        "            augmented_inklabel_g = None\n",
        "\n",
        "        if self.noise:\n",
        "            noise_t = torch.randn_like(augmented_input_g)\n",
        "            noise_t *= self.noise\n",
        "\n",
        "            augmented_input_g += noise_t\n",
        "\n",
        "        return augmented_input_g, augmented_label_g > 0.5, augmented_inklabel_g > 0.5 if inklabel_g is not None else None\n",
        "\n",
        "\n",
        "    def _build2dTransformMatrix(self):\n",
        "        transform_t = torch.eye(3)\n",
        "\n",
        "        for i in range(2):\n",
        "            if self.flip:\n",
        "                if random.random() > 0.5:\n",
        "                    transform_t[i,i] *= -1\n",
        "\n",
        "            if self.offset:\n",
        "                offset_float = self.offset\n",
        "                random_float = (random.random() * 2 - 1)\n",
        "                transform_t[2,i] = offset_float * random_float\n",
        "\n",
        "            if self.scale:\n",
        "                scale_float = self.scale\n",
        "                random_float = (random.random() * 2 - 1)\n",
        "                transform_t[i,i] *= 1.0 + scale_float * random_float\n",
        "\n",
        "        if self.rotate:\n",
        "            angle_rad = random.random() * math.pi * 2\n",
        "            s = math.sin(angle_rad)\n",
        "            c = math.cos(angle_rad)\n",
        "\n",
        "            rotation_t = torch.tensor([\n",
        "                [c, -s, 0],\n",
        "                [s, c, 0],\n",
        "                [0, 0, 1]])\n",
        "\n",
        "            transform_t @= rotation_t\n",
        "\n",
        "        return transform_t"
      ],
      "metadata": {
        "id": "YsoHxU0POsXR"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# im = dataset[0][1].unsqueeze(0)\n",
        "# label = dataset[0][2].unsqueeze(0)\n",
        "# ink = dataset[0][3].unsqueeze(0)\n",
        "\n",
        "# a, b, c = segmentation_augmentation(im, label, ink, flip=True, offset=0.03, scale=0.2, rotate=True, noise=25.0)\n",
        "\n",
        "# # import matplotlib.pyplot as plt\n",
        "# image_np = a.squeeze(0).numpy()\n",
        "\n",
        "# fig, axs = plt.subplots(1, 3)\n",
        "# for i in range(3):\n",
        "#     axs[i].imshow(image_np[i], cmap='gray')\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "mi82FDW8jS6p"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import collections\n",
        "import copy\n",
        "import datetime\n",
        "import gc\n",
        "import time\n",
        "\n",
        "# import torch\n",
        "import numpy as np\n",
        "\n",
        "# from util.logconf import logging\n",
        "# log = logging.getLogger(__name__)\n",
        "# # log.setLevel(logging.WARN)\n",
        "# # log.setLevel(logging.INFO)\n",
        "# log.setLevel(logging.DEBUG)\n",
        "\n",
        "\n",
        "def enumerateWithEstimate(\n",
        "        iter,\n",
        "        desc_str,\n",
        "        start_ndx=0,\n",
        "        print_ndx=4,\n",
        "        backoff=None,\n",
        "        iter_len=None,\n",
        "):\n",
        "    \n",
        "    if iter_len is None:\n",
        "        iter_len = len(iter)\n",
        "\n",
        "    if backoff is None:\n",
        "        backoff = 2\n",
        "        while backoff ** 7 < iter_len:\n",
        "            backoff *= 2\n",
        "\n",
        "    assert backoff >= 2\n",
        "    while print_ndx < start_ndx * backoff:\n",
        "        print_ndx *= backoff\n",
        "\n",
        "    log.warning(\"{} ----/{}, starting\".format(\n",
        "        desc_str,\n",
        "        iter_len,\n",
        "    ))\n",
        "    start_ts = time.time()\n",
        "    for (current_ndx, item) in enumerate(iter):\n",
        "        yield (current_ndx, item)\n",
        "        if current_ndx == print_ndx:\n",
        "            # ... <1>\n",
        "            duration_sec = ((time.time() - start_ts)\n",
        "                            / (current_ndx - start_ndx + 1)\n",
        "                            * (iter_len-start_ndx)\n",
        "                            )\n",
        "\n",
        "            done_dt = datetime.datetime.fromtimestamp(start_ts + duration_sec)\n",
        "            done_td = datetime.timedelta(seconds=duration_sec)\n",
        "\n",
        "            log.info(\"{} {:-4}/{}, done at {}, {}\".format(\n",
        "                desc_str,\n",
        "                current_ndx,\n",
        "                iter_len,\n",
        "                str(done_dt).rsplit('.', 1)[0],\n",
        "                str(done_td).rsplit('.', 1)[0],\n",
        "            ))\n",
        "\n",
        "            print_ndx *= backoff\n",
        "\n",
        "        if current_ndx + 1 == start_ndx:\n",
        "            start_ts = time.time()\n",
        "\n",
        "    log.warning(\"{} ----/{}, done at {}\".format(\n",
        "        desc_str,\n",
        "        iter_len,\n",
        "        str(datetime.datetime.now()).rsplit('.', 1)[0],\n",
        "    ))"
      ],
      "metadata": {
        "id": "xhI7RatW98-t"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for i, sample in enumerateWithEstimate(train_dataset, \"Processing samples\"):\n",
        "#     print(f\"Sample {i}:\")\n",
        "#     print(f\"  ID: {sample[0]}\")\n",
        "#     print(f\"  Image shape: {sample[1].shape}\")\n",
        "#     print(f\"  Label shape: {sample[2][0]}\")"
      ],
      "metadata": {
        "id": "uCO2e2Iyty-t"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import hashlib\n",
        "import os\n",
        "import shutil\n",
        "import socket\n",
        "import sys\n",
        "\n",
        "import numpy as np\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim\n",
        "\n",
        "from torch.optim import SGD, Adam\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# from util.util import enumerateWithEstimate\n",
        "# from .dsets import Luna2dSegmentationDataset, TrainingLuna2dSegmentationDataset, getCt\n",
        "# from util.logconf import logging\n",
        "# from .model import UNetWrapper, SegmentationAugmentation\n",
        "\n",
        "log = logging.getLogger(__name__)\n",
        "# log.setLevel(logging.WARN)\n",
        "# log.setLevel(logging.INFO)\n",
        "log.setLevel(logging.DEBUG)\n",
        "\n",
        "# Used for computeClassificationLoss and logMetrics to index into metrics_t/metrics_a\n",
        "# METRICS_LABEL_NDX = 0\n",
        "METRICS_LOSS_NDX = 1\n",
        "METRICS_TP_NDX = 7\n",
        "METRICS_FN_NDX = 8\n",
        "METRICS_FP_NDX = 9\n",
        "\n",
        "METRICS_SIZE = 10\n",
        "\n",
        "class SegmentationTrainingApp:\n",
        "    def __init__(self, augmentation_dict=None):\n",
        "        if augmentation_dict is None:\n",
        "            self.augmentation_dict = {\n",
        "             'flip': True,\n",
        "              'offset': 0.1,\n",
        "              'scale': 0.2,\n",
        "              'rotate': True, \n",
        "              'noise': 25.0\n",
        "             }\n",
        "        else:\n",
        "            self.augmentation_dict = augmentation_dict\n",
        "\n",
        "        self.time_str = datetime.datetime.now().strftime('%Y-%m-%d_%H.%M.%S')\n",
        "        self.totalTrainingSamples_count = 0\n",
        "        self.trn_writer = None\n",
        "        self.val_writer = None\n",
        "        self.epochs = 1\n",
        "        self.batch_size = 4\n",
        "\n",
        "        # self.augmentation_dict = self.augmentation_options\n",
        "\n",
        "        self.use_cuda = torch.cuda.is_available()\n",
        "        self.device = torch.device(\"cuda\" if self.use_cuda else \"cpu\")\n",
        "\n",
        "        self.segmentation_model, self.augmentation_model = self.initModel()\n",
        "        self.optimizer = self.initOptimizer()\n",
        "\n",
        "    def initModel(self):\n",
        "        segmentation_model = UNetWrapper(\n",
        "            in_channels=3,\n",
        "            n_classes=1,\n",
        "            depth=3,\n",
        "            wf=4,\n",
        "            padding=True,\n",
        "            batch_norm=True,\n",
        "            up_mode='upconv',\n",
        "        )\n",
        "\n",
        "        augmentation_model = SegmentationAugmentation(**self.augmentation_dict)\n",
        "\n",
        "        if self.use_cuda:\n",
        "            log.info(\"Using CUDA; {} devices.\".format(torch.cuda.device_count()))\n",
        "            if torch.cuda.device_count() > 1:\n",
        "                segmentation_model = nn.DataParallel(segmentation_model)\n",
        "                augmentation_model = nn.DataParallel(augmentation_model)\n",
        "            segmentation_model = segmentation_model.to(self.device)\n",
        "            augmentation_model = augmentation_model.to(self.device)\n",
        "\n",
        "        return segmentation_model, augmentation_model\n",
        "\n",
        "    def initOptimizer(self):\n",
        "        return Adam(self.segmentation_model.parameters())\n",
        "\n",
        "    def initTrainDl(self):\n",
        "        train_ds = TrainingSegmentationDataset('train', (256, 256), 3, True)\n",
        "\n",
        "        batch_size = 4\n",
        "        if self.use_cuda:\n",
        "            batch_size *= torch.cuda.device_count()\n",
        "\n",
        "        train_dl = DataLoader(\n",
        "            train_ds,\n",
        "            batch_size=batch_size,\n",
        "            num_workers=4,\n",
        "            pin_memory=self.use_cuda,\n",
        "        )\n",
        "\n",
        "        return train_dl\n",
        "\n",
        "    def initTestDl(self):\n",
        "        test_ds = ValidationSegmentationDataset('test', (512, 512), 3, True)\n",
        "\n",
        "        batch_size = 4\n",
        "        if self.use_cuda:\n",
        "            batch_size *= torch.cuda.device_count()\n",
        "\n",
        "        test_dl = DataLoader(\n",
        "            test_ds,\n",
        "            batch_size=batch_size,\n",
        "            num_workers=4,\n",
        "            pin_memory=self.use_cuda,\n",
        "        )\n",
        "\n",
        "        return test_dl\n",
        "\n",
        "\n",
        "    def main(self):\n",
        "        # log.info(\"Starting {}, {}\".format(type(self).__name__, self.cli_args))\n",
        "\n",
        "        train_dl = self.initTrainDl()\n",
        "        test_dl = self.initTestDl() # remove validation dataset\n",
        "        # print(train_dl.dataset[0][1].shape)\n",
        "\n",
        "        best_score = 0.0\n",
        "        # self.validation_cadence = 5 # remove validation cadence\n",
        "        for epoch_ndx in range(1, self.epochs + 1):\n",
        "            log.info(\"Epoch {} of {}, {} batches of size {}*{}\".format(\n",
        "                epoch_ndx,\n",
        "                self.epochs,\n",
        "                len(train_dl),\n",
        "                len(test_dl), # remove validation dataset\n",
        "                self.batch_size,\n",
        "                (torch.cuda.device_count() if self.use_cuda else 1),\n",
        "            ))\n",
        "\n",
        "        trnMetrics_t = self.doTraining(epoch_ndx, train_dl)\n",
        "        self.logMetrics(epoch_ndx, 'trn', trnMetrics_t)\n",
        "\n",
        "            # if epoch_ndx == 1 or epoch_ndx % self.validation_cadence == 0:\n",
        "            #     # if validation is wanted\n",
        "            #     valMetrics_t = self.doTesting(epoch_ndx, val_dl)\n",
        "            #     score = self.logMetrics(epoch_ndx, 'val', valMetrics_t)\n",
        "            #     best_score = max(score, best_score)\n",
        "\n",
        "            # self.saveModel('seg', epoch_ndx, score == best_score)\n",
        "\n",
        "            # self.logImages(epoch_ndx, 'trn', train_dl)\n",
        "            # self.logImages(epoch_ndx, 'val', val_dl)\n",
        "\n",
        "        self.trn_writer.close()\n",
        "        # self.val_writer.close()\n",
        "\n",
        "    def doTraining(self, epoch_ndx, train_dl):\n",
        "        # print(len(train_dl.dataset))\n",
        "        trnMetrics_g = torch.zeros(METRICS_SIZE, len(train_dl.dataset), device=self.device)\n",
        "        self.segmentation_model.train()\n",
        "        # train_dl.dataset.shuffleSamples()\n",
        "\n",
        "        batch_iter = enumerateWithEstimate(\n",
        "            train_dl,\n",
        "            \"E{} Training\".format(epoch_ndx),\n",
        "            start_ndx=train_dl.num_workers,\n",
        "        )\n",
        "        for batch_ndx, batch_tup in batch_iter:\n",
        "            self.optimizer.zero_grad()\n",
        "\n",
        "            loss_var = self.computeBatchLoss(batch_ndx, batch_tup, train_dl.batch_size, trnMetrics_g)\n",
        "            loss_var.backward()\n",
        "\n",
        "            self.optimizer.step()\n",
        "\n",
        "        self.totalTrainingSamples_count += trnMetrics_g.size(1)\n",
        "\n",
        "        return trnMetrics_g.to('cpu')\n",
        "\n",
        "    def doTesting(self, epoch_ndx, test_dl):\n",
        "        with torch.no_grad():\n",
        "            testMetrics_g = torch.zeros(METRICS_SIZE, len(test_dl.dataset), device=self.device)\n",
        "            self.segmentation_model.eval()\n",
        "\n",
        "            batch_iter = enumerateWithEstimate(\n",
        "                test_dl,\n",
        "                \"E{} Testing \".format(epoch_ndx),\n",
        "                start_ndx=test_dl.num_workers,\n",
        "            )\n",
        "            for batch_ndx, batch_tup in batch_iter:\n",
        "                self.computeBatchLoss(batch_ndx, batch_tup, test_dl.batch_size, testMetrics_g)\n",
        "\n",
        "        return testMetrics_g.to('cpu')\n",
        "\n",
        "    def computeBatchLoss(self, batch_ndx, batch_tup, batch_size, metrics_g,\n",
        "                         classificationThreshold=0.5):\n",
        "        input_t, label_t, ink_t, series_list, _slice_ndx_list = batch_tup\n",
        "\n",
        "        input_g = input_t.to(self.device, non_blocking=True)\n",
        "        label_g = label_t.to(self.device, non_blocking=True)\n",
        "        ink_g = ink_t.to(self.device, non_blocking=True)\n",
        "\n",
        "        if self.segmentation_model.training and self.augmentation_dict:\n",
        "            input_g, label_g, ink_g = self.augmentation_model(input_g, label_g, ink_g)\n",
        "\n",
        "        prediction_g = self.segmentation_model(input_g)\n",
        "        combined_label_mask_t = label_g | ink_g\n",
        "        diceLoss_g = self.diceLoss(prediction_g, combined_label_mask_t)\n",
        "        fnLoss_g = self.diceLoss(prediction_g * combined_label_mask_t, combined_label_mask_t)\n",
        "\n",
        "        start_ndx = batch_ndx * batch_size\n",
        "        end_ndx = start_ndx + input_t.size(0)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            predictionBool_g = (prediction_g[:, 0:1]\n",
        "                                > classificationThreshold).to(torch.float32)\n",
        "\n",
        "            tp = (     predictionBool_g *  label_g).sum(dim=[1,2,3])\n",
        "            fn = ((1 - predictionBool_g) *  label_g).sum(dim=[1,2,3])\n",
        "            fp = (     predictionBool_g * (~label_g)).sum(dim=[1,2,3])\n",
        "\n",
        "            metrics_g[METRICS_LOSS_NDX, start_ndx:end_ndx] = diceLoss_g\n",
        "            metrics_g[METRICS_TP_NDX, start_ndx:end_ndx] = tp\n",
        "            metrics_g[METRICS_FN_NDX, start_ndx:end_ndx] = fn\n",
        "            metrics_g[METRICS_FP_NDX, start_ndx:end_ndx] = fp\n",
        "\n",
        "        return diceLoss_g.mean() + fnLoss_g.mean() * 8\n",
        "\n",
        "    def diceLoss(self, prediction_g, label_g, epsilon=1):\n",
        "        diceLabel_g = label_g.sum(dim=[1,2,3])\n",
        "        dicePrediction_g = prediction_g.sum(dim=[1,2,3])\n",
        "        diceCorrect_g = (prediction_g * label_g).sum(dim=[1,2,3])\n",
        "\n",
        "        diceRatio_g = (2 * diceCorrect_g + epsilon) \\\n",
        "            / (dicePrediction_g + diceLabel_g + epsilon)\n",
        "\n",
        "        return 1 - diceRatio_g\n",
        "\n",
        "    # def logImages(self, epoch_ndx, mode_str, dl):\n",
        "    #     self.segmentation_model.eval()\n",
        "\n",
        "    #     images = sorted(dl.dataset.series_list)[:12]\n",
        "    #     for series_ndx, series_uid in enumerate(images):\n",
        "    #         ct = getCt(series_uid)\n",
        "\n",
        "    #         for slice_ndx in range(6):\n",
        "    #             ct_ndx = slice_ndx * (ct.hu_a.shape[0] - 1) // 5\n",
        "    #             sample_tup = dl.dataset.getitem_fullSlice(series_uid, ct_ndx)\n",
        "\n",
        "    #             ct_t, label_t, series_uid, ct_ndx = sample_tup\n",
        "\n",
        "    #             input_g = ct_t.to(self.device).unsqueeze(0)\n",
        "    #             label_g = pos_g = label_t.to(self.device).unsqueeze(0)\n",
        "\n",
        "    #             prediction_g = self.segmentation_model(input_g)[0]\n",
        "    #             prediction_a = prediction_g.to('cpu').detach().numpy()[0] > 0.5\n",
        "    #             label_a = label_g.cpu().numpy()[0][0] > 0.5\n",
        "\n",
        "    #             ct_t[:-1,:,:] /= 2000\n",
        "    #             ct_t[:-1,:,:] += 0.5\n",
        "\n",
        "    #             ctSlice_a = ct_t[dl.dataset.contextSlices_count].numpy()\n",
        "\n",
        "    #             image_a = np.zeros((512, 512, 3), dtype=np.float32)\n",
        "    #             image_a[:,:,:] = ctSlice_a.reshape((512,512,1))\n",
        "    #             image_a[:,:,0] += prediction_a & (1 - label_a)\n",
        "    #             image_a[:,:,0] += (1 - prediction_a) & label_a\n",
        "    #             image_a[:,:,1] += ((1 - prediction_a) & label_a) * 0.5\n",
        "\n",
        "    #             image_a[:,:,1] += prediction_a & label_a\n",
        "    #             image_a *= 0.5\n",
        "    #             image_a.clip(0, 1, image_a)\n",
        "\n",
        "    #             writer = getattr(self, mode_str + '_writer')\n",
        "    #             writer.add_image(\n",
        "    #                 f'{mode_str}/{series_ndx}_prediction_{slice_ndx}',\n",
        "    #                 image_a,\n",
        "    #                 self.totalTrainingSamples_count,\n",
        "    #                 dataformats='HWC',\n",
        "    #             )\n",
        "\n",
        "    #             if epoch_ndx == 1:\n",
        "    #                 image_a = np.zeros((512, 512, 3), dtype=np.float32)\n",
        "    #                 image_a[:,:,:] = ctSlice_a.reshape((512,512,1))\n",
        "    #                 # image_a[:,:,0] += (1 - label_a) & lung_a # Red\n",
        "    #                 image_a[:,:,1] += label_a  # Green\n",
        "    #                 # image_a[:,:,2] += neg_a  # Blue\n",
        "\n",
        "    #                 image_a *= 0.5\n",
        "    #                 image_a[image_a < 0] = 0\n",
        "    #                 image_a[image_a > 1] = 1\n",
        "    #                 writer.add_image(\n",
        "    #                     '{}/{}_label_{}'.format(\n",
        "    #                         mode_str,\n",
        "    #                         series_ndx,\n",
        "    #                         slice_ndx,\n",
        "    #                     ),\n",
        "    #                     image_a,\n",
        "    #                     self.totalTrainingSamples_count,\n",
        "    #                     dataformats='HWC',\n",
        "    #                 )\n",
        "    #             # This flush prevents TB from getting confused about which\n",
        "    #             # data item belongs where.\n",
        "    #             writer.flush()\n",
        "\n",
        "    def logMetrics(self, epoch_ndx, mode_str, metrics_t):\n",
        "        log.info(\"E{} {}\".format(\n",
        "            epoch_ndx,\n",
        "            type(self).__name__,\n",
        "        ))\n",
        "\n",
        "        metrics_a = metrics_t.detach().numpy()\n",
        "        sum_a = metrics_a.sum(axis=1)\n",
        "        assert np.isfinite(metrics_a).all()\n",
        "\n",
        "        allLabel_count = sum_a[METRICS_TP_NDX] + sum_a[METRICS_FN_NDX]\n",
        "\n",
        "        metrics_dict = {}\n",
        "        metrics_dict['loss/all'] = metrics_a[METRICS_LOSS_NDX].mean()\n",
        "\n",
        "        metrics_dict['percent_all/tp'] = \\\n",
        "            sum_a[METRICS_TP_NDX] / (allLabel_count or 1) * 100\n",
        "        metrics_dict['percent_all/fn'] = \\\n",
        "            sum_a[METRICS_FN_NDX] / (allLabel_count or 1) * 100\n",
        "        metrics_dict['percent_all/fp'] = \\\n",
        "            sum_a[METRICS_FP_NDX] / (allLabel_count or 1) * 100\n",
        "\n",
        "\n",
        "        precision = metrics_dict['pr/precision'] = sum_a[METRICS_TP_NDX] \\\n",
        "            / ((sum_a[METRICS_TP_NDX] + sum_a[METRICS_FP_NDX]) or 1)\n",
        "        recall    = metrics_dict['pr/recall']    = sum_a[METRICS_TP_NDX] \\\n",
        "            / ((sum_a[METRICS_TP_NDX] + sum_a[METRICS_FN_NDX]) or 1)\n",
        "\n",
        "        metrics_dict['pr/f1_score'] = 2 * (precision * recall) \\\n",
        "            / ((precision + recall) or 1)\n",
        "\n",
        "        log.info((\"E{} {:8} \"\n",
        "                 + \"{loss/all:.4f} loss, \"\n",
        "                 + \"{pr/precision:.4f} precision, \"\n",
        "                 + \"{pr/recall:.4f} recall, \"\n",
        "                 + \"{pr/f1_score:.4f} f1 score\"\n",
        "                  ).format(\n",
        "            epoch_ndx,\n",
        "            mode_str,\n",
        "            **metrics_dict,\n",
        "        ))\n",
        "        log.info((\"E{} {:8} \"\n",
        "                  + \"{loss/all:.4f} loss, \"\n",
        "                  + \"{percent_all/tp:-5.1f}% tp, {percent_all/fn:-5.1f}% fn, {percent_all/fp:-9.1f}% fp\"\n",
        "        ).format(\n",
        "            epoch_ndx,\n",
        "            mode_str + '_all',\n",
        "            **metrics_dict,\n",
        "        ))\n",
        "\n",
        "        # self.initTensorboardWriters()\n",
        "        writer = getattr(self, mode_str + '_writer')\n",
        "\n",
        "        prefix_str = 'seg_'\n",
        "        print(metrics_dict)\n",
        "        # for key, value in metrics_dict.items():\n",
        "        #     writer.add_scalar(prefix_str + key, value, self.totalTrainingSamples_count)\n",
        "\n",
        "        # writer.flush()\n",
        "\n",
        "        score = metrics_dict['pr/recall']\n",
        "\n",
        "        return score\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "gLZsKaYQ6hU6"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    SegmentationTrainingApp().main()"
      ],
      "metadata": {
        "id": "2-q_oO2GZh3R",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f97bb9fe-005e-4c1f-baa9-ee136e355225"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Data/Vesuvisius/train/1\n",
            "[0, 1, 2, 3, 4]\n",
            "[[False False False]\n",
            " [False False False]\n",
            " [False False False]]\n",
            "/content/drive/MyDrive/Data/Vesuvisius/train/2\n",
            "[0, 1, 2, 3, 4]\n",
            "[[False False False]\n",
            " [False False False]\n",
            " [False False False]]\n",
            "/content/drive/MyDrive/Data/Vesuvisius/train/3\n",
            "[0, 1, 2, 3, 4]\n",
            "[[False False False]\n",
            " [False False False]\n",
            " [False False False]]\n",
            "/content/drive/MyDrive/Data/Vesuvisius/test/a\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0, 1, 2, 3, 4]\n",
            "/content/drive/MyDrive/Data/Vesuvisius/test/b\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2023-06-15 03:27:24,831 INFO     pid:532 __main__:133:main Epoch 1 of 1, 4 batches of size 3*4\n",
            "2023-06-15 03:27:24,834 WARNING  pid:532 __main__:038:enumerateWithEstimate E1 Training ----/4, starting\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0, 1, 2, 3, 4]\n",
            "/content/drive/MyDrive/Data/Vesuvisius/train/1\n",
            "/content/drive/MyDrive/Data/Vesuvisius/train/1/content/drive/MyDrive/Data/Vesuvisius/train/2\n",
            "\n",
            "/content/drive/MyDrive/Data/Vesuvisius/train/3\n",
            "[0, 1, 2, 3, 4]\n",
            "[[False False False]\n",
            " [False False False]\n",
            " [False False False]]\n",
            "[0, 1, 2, 3, 4]\n",
            "[0, 1, 2, 3, 4]\n",
            "[[False False False]\n",
            " [False False False]\n",
            " [False False False]]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-54-dfd47c5c90e1>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mSegmentationTrainingApp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-53-ab6d66f460ea>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    140\u001b[0m             ))\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m         \u001b[0mtrnMetrics_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdoTraining\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_ndx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogMetrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_ndx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'trn'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrnMetrics_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-53-ab6d66f460ea>\u001b[0m in \u001b[0;36mdoTraining\u001b[0;34m(self, epoch_ndx, train_dl)\u001b[0m\n\u001b[1;32m    168\u001b[0m             \u001b[0mstart_ndx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_dl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m         )\n\u001b[0;32m--> 170\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch_ndx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_tup\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch_iter\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-51-2537275649de>\u001b[0m in \u001b[0;36menumerateWithEstimate\u001b[0;34m(iter, desc_str, start_ndx, print_ndx, backoff, iter_len)\u001b[0m\n\u001b[1;32m     41\u001b[0m     ))\n\u001b[1;32m     42\u001b[0m     \u001b[0mstart_ts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcurrent_ndx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m         \u001b[0;32myield\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcurrent_ndx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcurrent_ndx\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mprint_ndx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    631\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 633\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    634\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    635\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1343\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1344\u001b[0m                 \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1345\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1346\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1347\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1369\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1370\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1371\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1372\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1373\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    642\u001b[0m             \u001b[0;31m# instantiate since we don't know how to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    643\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 644\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    645\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\", line 51, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\", line 51, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"<ipython-input-44-b25335e2a30a>\", line 96, in __getitem__\n    return self.getitem_trainCrop(series_uid, slice_ndx)\n  File \"<ipython-input-44-b25335e2a30a>\", line 115, in getitem_trainCrop\n    ct_t[i] = torch.from_numpy(ct.stacked[context_ndx].astype(np.float32))\nRuntimeError: The expanded size of the tensor (256) must match the existing size (3) at non-singleton dimension 1.  Target sizes: [256, 256].  Tensor sizes: [3, 3]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[False False False]\n",
            " [False False False]\n",
            " [False False False]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "j5PPuhdgjE1e"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}